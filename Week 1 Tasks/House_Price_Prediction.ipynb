{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZn6nfFeu5Na"
      },
      "outputs": [],
      "source": [
        "# Cell 1. installs and imports\n",
        "# run this first cell in Colab\n",
        "!pip install -q kagglehub\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import joblib\n",
        "\n",
        "sns.set(style='whitegrid')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "KAGGLEHUB_PATH = \"/root/.cache/kagglehub/datasets/harishkumardatalab/housing-price-prediction/versions/1\"\n",
        "\n",
        "files = os.listdir(KAGGLEHUB_PATH)\n",
        "print(\"files in dataset folder:\", files)\n",
        "\n",
        "csv_files = [f for f in files if f.lower().endswith(\".csv\")]\n",
        "if not csv_files:\n",
        "    raise Exception(\"no csv found in the dataset folder. list files above and set path correctly\")\n",
        "\n",
        "csv_path = os.path.join(KAGGLEHUB_PATH, csv_files[0])\n",
        "print(\"using csv:\", csv_path)\n",
        "\n",
        "house = pd.read_csv(csv_path)\n",
        "house.shape, house.columns.tolist()"
      ],
      "metadata": {
        "id": "RdY7LE0ivEE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# this is the path KaggleHub gave you\n",
        "path = \"/root/.cache/kagglehub/datasets/harishkumardatalab/housing-price-prediction/versions/1\"\n",
        "\n",
        "# list files to see what's inside\n",
        "files = os.listdir(path)\n",
        "print(\"Files inside dataset folder:\", files)\n",
        "\n",
        "# auto-detect the csv file\n",
        "csv_files = [f for f in files if f.endswith(\".csv\")]\n",
        "\n",
        "if len(csv_files) == 0:\n",
        "    raise Exception(\"No CSV file found in dataset folder.\")\n",
        "\n",
        "csv_path = os.path.join(path, csv_files[0])\n",
        "print(\"Using file:\", csv_path)\n",
        "\n",
        "# load dataset\n",
        "house = pd.read_csv(csv_path)\n",
        "\n",
        "# preview\n",
        "house.head()\n"
      ],
      "metadata": {
        "id": "PXiOLcQsvndM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# . basic info, missing values\n",
        "print(\"shape:\", house.shape)\n",
        "print(\"\\nmissing per column:\")\n",
        "print(house.isnull().sum().sort_values(ascending=False).head(30))\n",
        "\n",
        "print(\"\\ndtypes:\")\n",
        "print(house.dtypes)\n"
      ],
      "metadata": {
        "id": "u2934yS4v_18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# target detection and quick fixes\n",
        "# common target names, add more if needed\n",
        "possible_targets = ['price', 'Price', 'SalePrice', 'sale_price', 'Sale_Price', 'house_price']\n",
        "target = None\n",
        "for t in possible_targets:\n",
        "    if t in house.columns:\n",
        "        target = t\n",
        "        break\n",
        "\n",
        "# fallback: if dataset has exactly one numeric column that looks like price name, attempt heuristics\n",
        "if target is None:\n",
        "    # try column with highest variance and numeric as a guess for target\n",
        "    numeric_cols = house.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if len(numeric_cols) > 0:\n",
        "        guess = house[numeric_cols].var().sort_values(ascending=False).index[0]\n",
        "        target = guess\n",
        "        print(\"no obvious price column found, guessing target:\", target)\n",
        "    else:\n",
        "        raise Exception(\"no numeric columns found to act as target\")\n",
        "\n",
        "print(\"target column set to:\", target)\n",
        "\n",
        "# drop obviously useless columns if present\n",
        "drop_cols = [c for c in ['Id', 'id', 'ID', 'index'] if c in house.columns]\n",
        "if drop_cols:\n",
        "    house.drop(columns=drop_cols, inplace=True)\n",
        "    print(\"dropped:\", drop_cols)\n"
      ],
      "metadata": {
        "id": "D8HBpPgCwf99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploratory visuals (small, quick)\n",
        "# correlation heatmap of numeric features\n",
        "num = house.select_dtypes(include=[np.number]).copy()\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(num.corr(), cmap='coolwarm', center=0)\n",
        "plt.title(\"numeric feature correlation\")\n",
        "plt.show()\n",
        "\n",
        "# distribution of target\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.histplot(house[target].dropna(), kde=True)\n",
        "plt.title(\"target distribution\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6psigp2zwk9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing plan\n",
        "# - separate numeric and categorical\n",
        "# - impute missing values\n",
        "# - power transform target if strongly skewed\n",
        "# - one-hot encode categorical\n",
        "# - scale numeric features\n",
        "\n",
        "# simple heuristics for identifying columns\n",
        "numeric_features = house.select_dtypes(include=[np.number]).columns.tolist()\n",
        "if target in numeric_features:\n",
        "    numeric_features.remove(target)\n",
        "categorical_features = house.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "print(\"numeric features:\", len(numeric_features))\n",
        "print(\"categorical features:\", len(categorical_features))\n"
      ],
      "metadata": {
        "id": "9HdXt4f-woIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build preprocessing pipelines\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot',  pd.get_dummies) # placeholder, will handle separately\n",
        "])\n",
        "\n",
        "# Because sklearn ColumnTransformer can't use pandas.get_dummies directly, we'll do get_dummies first for simplicity\n",
        "house_prep = house.copy()\n",
        "\n",
        "# impute simple missing numeric values with median now, because get_dummies changes structure\n",
        "for col in numeric_features:\n",
        "    if house_prep[col].isnull().any():\n",
        "        med = house_prep[col].median()\n",
        "        house_prep[col].fillna(med, inplace=True)\n",
        "\n",
        "# fill categorical missing with 'missing'\n",
        "for col in categorical_features:\n",
        "    house_prep[col].fillna('missing', inplace=True)\n",
        "\n",
        "# one-hot encode categoricals\n",
        "if categorical_features:\n",
        "    house_prep = pd.get_dummies(house_prep, columns=categorical_features, drop_first=True)\n",
        "\n",
        "print(\"shape after one-hot:\", house_prep.shape)\n"
      ],
      "metadata": {
        "id": "KKWm3RawwwoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#final X, y, optional log transform target if skewed\n",
        "X = house_prep.drop(columns=[target])\n",
        "y = house_prep[target].copy()\n",
        "\n",
        "# examine skew\n",
        "skewness = y.skew()\n",
        "print(\"target skewness:\", skewness)\n",
        "apply_log = False\n",
        "if skewness > 1.0:\n",
        "    # power transform target for better regression performance\n",
        "    y = np.log1p(y)\n",
        "    apply_log = True\n",
        "    print(\"applied log1p transform to target\")\n",
        "\n",
        "# train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(\"train shape\", X_train.shape, \"test shape\", X_test.shape)\n"
      ],
      "metadata": {
        "id": "n0L7avujw1Om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#benchmark model: Linear Regression\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "preds_lr = lr.predict(X_test)\n",
        "\n",
        "# if we transformed target, invert for metrics display\n",
        "def invert_target(arr):\n",
        "    return np.expm1(arr) if apply_log else arr\n",
        "\n",
        "y_test_inv = invert_target(y_test)\n",
        "preds_lr_inv = invert_target(preds_lr)\n",
        "\n",
        "print(\"Linear Regression metrics:\")\n",
        "print(\"MAE:\", mean_absolute_error(y_test_inv, preds_lr_inv))\n",
        "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test_inv, preds_lr_inv)))\n",
        "print(\"R2:\", r2_score(y_test_inv, preds_lr_inv))"
      ],
      "metadata": {
        "id": "ihwK22oew6_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gradient Boosting with basic CV tuning\n",
        "gbr = GradientBoostingRegressor(random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'learning_rate': [0.05, 0.1],\n",
        "    'max_depth': [3, 5]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(gbr, param_grid, cv=4, scoring='neg_mean_absolute_error', n_jobs=-1, verbose=1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"best params:\", grid.best_params_)\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "# predict and invert\n",
        "preds_gbr = best_model.predict(X_test)\n",
        "preds_gbr_inv = invert_target(preds_gbr)\n",
        "\n",
        "print(\"\\nGradient Boosting metrics on test set:\")\n",
        "print(\"MAE:\", mean_absolute_error(y_test_inv, preds_gbr_inv))\n",
        "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test_inv, preds_gbr_inv)))\n",
        "print(\"R2:\", r2_score(y_test_inv, preds_gbr_inv))"
      ],
      "metadata": {
        "id": "b9QSIXp_w_9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RandomForest for comparison\n",
        "rf = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n",
        "rf.fit(X_train, y_train)\n",
        "preds_rf = rf.predict(X_test)\n",
        "preds_rf_inv = invert_target(preds_rf)\n",
        "\n",
        "print(\"Random Forest metrics:\")\n",
        "print(\"MAE:\", mean_absolute_error(y_test_inv, preds_rf_inv))\n",
        "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test_inv, preds_rf_inv)))\n",
        "print(\"R2:\", r2_score(y_test_inv, preds_rf_inv))\n"
      ],
      "metadata": {
        "id": "C8FM_F2HxTv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#actual vs predicted plot for best model (gbr)\n",
        "plt.figure(figsize=(7,6))\n",
        "plt.scatter(y_test_inv, preds_gbr_inv, alpha=0.6)\n",
        "plt.plot([y_test_inv.min(), y_test_inv.max()], [y_test_inv.min(), y_test_inv.max()], linestyle='--')\n",
        "plt.xlabel(\"Actual Price\")\n",
        "plt.ylabel(\"Predicted Price\")\n",
        "plt.title(\"Actual vs Predicted - Gradient Boosting\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "l0lA-bQUx-tH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# feature importance (top 20)\n",
        "if hasattr(best_model, 'feature_importances_'):\n",
        "    fi = best_model.feature_importances_\n",
        "    feat_names = X.columns\n",
        "    fi_df = pd.DataFrame({'feature': feat_names, 'importance': fi})\n",
        "    fi_df = fi_df.sort_values('importance', ascending=False).head(20)\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.barplot(data=fi_df, x='importance', y='feature')\n",
        "    plt.title(\"Top 20 Feature Importances\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"model has no feature_importances_ attribute\")\n"
      ],
      "metadata": {
        "id": "B01TvMzByAp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the trained best model and the preprocessing metadata\n",
        "MODEL_OUT = \"gbr_model.joblib\"\n",
        "joblib.dump({\n",
        "    'model': best_model,\n",
        "    'columns': X.columns.tolist(),\n",
        "    'apply_log_target': apply_log\n",
        "}, MODEL_OUT)\n",
        "print(\"saved model artifact to\", MODEL_OUT)\n"
      ],
      "metadata": {
        "id": "bLDdb6fxyGTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# quick notes to add in your notebook or README\n",
        "notes = \"\"\"\n",
        "Task 6 completed, notes:\n",
        "- loaded KaggleHub csv directly from path\n",
        "- handled missing values with medians for numeric and 'missing' for categoricals\n",
        "- one-hot encoded categorical variables\n",
        "- optional log1p transform applied to target if skewed\n",
        "- evaluated LinearRegression, GradientBoosting and RandomForest\n",
        "- used GridSearchCV to tune GBR hyperparameters\n",
        "- saved best model to gbr_model.joblib\n",
        "\"\"\"\n",
        "print(notes)\n"
      ],
      "metadata": {
        "id": "zuHjpVuLyJjv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}